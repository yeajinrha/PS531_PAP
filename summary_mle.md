
Recall that statistical inference is a collection of procedures that allow us to learn about the unobserved. We infer about the unobserved using estimators and tests (including confidence intervals). We describe the observed using graphs and summaries like means, medians, standard deviations, coverage intervals which allow us to say things like "50% of the observed data lie in this range", etc...

At the beginning of the course we learned about *design-based* statistical inference where we do not observe how the treated units would have acted under control because each unit is only assigned to one experimental condition OR where we do not observe some population quantities (like the mean of the population) because we only have a sample. So estimated those quantities and/or tested hypotheses about those quantities. We engaged with the two main approaches here:

-   the test based approach to statistical inference (simplest version is Fisher. We practiced using it with hypotheses about potential outcomes --- i.e. to use statistical inference for causal inference.)
-   estimation based approach to statistical inference (simplest version is Neyman. We practiced using it to infer about unobserved averages and differences of average potential outcomes --- i.e. to use statistical inference for causal inference about average effects. We also practiced inferring about population averages or population relationships using sample averages and relationships.)

We just spent two weeks talking about about *model-based* statistical inference. What is it? Why use it?  How does it work? What are we inferring to when we use it?

Notice that by "model" we mean a simplified account of how observed **outcomes** are generated.[^1] So these are **data models**. They are not substantive causal models of relationships among abstract entities or concepts.  They are abstract, but tell the story of how a given set of outcomes took on values. So, again, "model based" does not mean "formal models" or "social choice models" or "agent based models" here. It just means that we have a probabilistic and structural model of how outcomes take on their values for example, we might say that an outcome $$y$$ for a unit $$i$$ arises from a Poisson process with rate $$\lambda_i$$ that itself depends on the units's $$x_{i,1}$$ and $$x_{i,2}$$ via some equation like $$\lambda=g(\beta_1 x_1 + \frac{\beta_2}{x_2} + \beta_3 \frac{x_2}{x_1} + \beta_4 x_1^2)$$. This model is called the "data generating process" for unit $$i$$. The joint probabilistic process for all of the units, is called the likelihood function (in the MLE case): \$$p(y_1,...,y_n\|\theta) = L(\theta\|y_1,...,y_n)\$$ .[^2]


[^1]: There are deeper connections to the design-based approach here, by the way. After all, we did statistical inference using models of experimental treatment assignment or models of sampling. It is just that link between model and reality could be quite tight in those cases --- where I claim that I used a Bernoulli procedure for random assignment (my model) and generated treatment assignments using `rbinom` or `simple_ra` for example. In principle the link could be looser --- say, treatment is assigned at random by physically drawing from an urn and I use something like `sample()` or `complete_ra` to approximate that process.

[^2]: Bayesian approaches to statistical inference are also model based and also depend on likelihood functions, but also on other probability-and-structure functions often called "priors".]


For example, we might model the process by which coins take on Heads versus Tails for one flip as $$p(y_1 = 1) = Bernoulli(\pi_1)$$ (the dgp for the first coin flip).  Covariates, $$x$$,  enters by governing the parameter $$\pi_i = g(b_0 + b_1 x_1 + b_2 x_2^2 + x_3/b_3)$$ where $$g()$$ is a link function. The link function tends to be a convenience --- to force the function combining x's to produce a value that is valid for the given dgp. (For example, $$\theta$$ in the Bernoulli pmf must be between 0 and 1. So g() has to transform some linear and/or non-linear function of x's into the 0 to 1 range otherwise you can't use that dgp.)

**What are we inferring to**?  One idea, is that we have an "(imaginary) super-population" generated by our dgp and likelihood process. Another idea is that we are inferring the value of the parameters that bring the posited data model (the likelihood function) closest to our data: we don't know what parameter this would be in advance, so we infer it using the set of data on hand. A different set of data might lead to a different inference, thus we have standard errors, distributions of estimates (often called "sampling distributions" but not because of sampling from an actual population of units, but rather a generator of data).

**How to justify a dgp choice --- including probabilistic process and structural specification? How to justify a choice of a link function?** 

 - Perhaps a formal theory implies a certain probabilistic or stochastic process or parameter value range for such a process, or maybe competing theories imply different stochastic processes (or different parameter ranges of the same process). So, can fit both models to data and compare their fit, etc.. (latent choice models of logit and multinomial logit)

- Reason by analogy ("The process I am studying is very much like a memoryless stochastic process of durations."): more or less weak. (Although opens the door to a more formalized structural model using stochastic processes that we know how to derive. See [the wiki on stochastic processes](https://en.wikipedia.org/wiki/Stochastic_process) and associated footnotes like  Geoffrey Grimmett; David Stirzaker (2001). Probability and Random Processes where the different stochastic processes are derived from more or less first principles in abstract form.)

- Reason for convenience in prediction ("I am not going to interpret coefficients. I just want predictions of the most convenient type."): not so weak. Here we only want, say, predictions between 0 and 1, or only non-negative integers, etc..

- Cookbook choice ("Outcome is binary so I used a logit model", "Outcome is a count, so I use a Poisson model"): pretty weak and underspecfied (why logit versus other link functions? why poisson versus other models?)


What does Cioffi-Revilla mean when he says: "The duration T, or lifetime of a government, can be considered a random variable. As such, T has a probability density function (pdf) f(t) and a cumulative density function (cdf)..."? So, Cioffi-Revilla here is thinking about cabinet duration as a stochastic process and he has a subtantive model of such processes that he wants to fit to it --- perhaps to see if it is a good or poor fit, or perhaps for prediction, or perhaps to compare with other accounts of this process.



**How would we assess the operating characteristics of estimators and tests in a model-based approach?** 

Recall that when we assess estimators and tests (including confidence intervals) in a design-based world, we repeat the element of the design that is repeatable. For example, using `DeclareDesign` we might `declare_sampling` or `declare_assignment` depending on the design and on what we are trying to learn about (we might use **both** if, for example, we are trying to learn about a Population Average Treatment Effect and we have a randomly sampled survey and a randomized experiment within that survey). 

What about model-based estimators and tests?

We had a few ideas:

  -   **Idea 1: make the super-population** Imagine we had a relatively small sample or rare and/or skewed and/or long-tailed outcomes or key explanatory variables/treatments and we worried that the large-sample or consistency properties of MLE would not be working in our case. This is **not** a good idea to ask whether our poisson model is better than say, a negative binomial model for some given data, because after all, the actual data is poisson. So, this next assesses the performance of our model-based estimator and perhaps tests in the best possible circumstance for it --- when the actual data is really IID Poisson.^[There is a version of this where the super-population is not so simple but you want to use a simple model to address it. So you might create a funky super-population and then assess the bias etc. of your model-based estimator based on it.]

    1. Make a big population that follows the presumed dgp like `pop <- declare_population(y=rpois(n=1000000, lambda=.2))`
    2. `samp <- declare_sampling(N=actual_size)` if actual_size is like 20.
    3. `estimator1 <- declare_estimator(model=glm(family=poisson()))`

  -   **Idea 2: Have a non-model based population.** `declare_sampling`, and estimators, etc.. (this is what we did in exploration10)

  -   **Idea 3:** Use your adjustment method (say you have strata from matching), then do `declare_assignment`. (this is like exploration8)

**What to use to guide choice of "best" model?**

-   $$R^2$$/AIC/BIC/Cross-Validation --- best fit (but not overfit) is better (we really care about out-of-sample fit for prediction purposes)
-   If you are doing statistical inference, then choose approaches that have good operating characteristics (bias, precision, power, coverage, false positive rate,etc..)
-   Also interpretability, communicability, computability, justifiability

**Why would we use a model-based approach if we had a randomized experiment?**

 - First answer, you wouldn't. If you have an experiment, then use the Z model, not the Y model. Which means no MLE. (see also Freedman 2008 on bias in logistic regression in randomized experiments for causal inferences about differences in log odds.)

 - There a couple of examples of randomized experiments with lots of non-randomly missing data, non-random non-compliance with treatment, etc.. , so you might want to use a model in such a case: see "Principal Stratification Approach to Broken Randomized Experiments: A Case Study of School Choice Vouchers in New York City" as one example.
