A few key pieces from today (that are mostly well repeated [on some slides that I made for other instructors to use](https://egap.github.io/learningdays-resources/Slides/hypothesistesting-slides.pdf) and an [an overview of hypothesis testing I wrote](https://egap.org/resource/10-things-to-know-about-hypothesis-testing/).

 - A hypothesis is a statement about the unobserved. We can't observe both outcomes under control and treatment for a given person $i$ so a hypothesis can be a statement about how outcomes *would* differ for that person *if we could see them*. A hypothesis can be about mean differences if we could treat everyone (which we can't). A hypothesis can be about a population quantity if we only have a sample. If you observe everything and thre is no other possible world, then you really don't hypothesize, you just observe and describe.
- A p-value reflects the information in your design as summarized by your test statistic as compared to **some distribution**. What is that distribution? What creates it? What describes it? 
  - **It depends:** in the simplest case, that we start with, the experimental design does this work. See Fisher 1935 Chapter 2 and his story about Muriel Bristol's tea tasting abilities. 
  - Distributions of things occur when you repeat them. I can distribute M&M's on the floor by throwing many on the floor. But one M&M on the floor is just an M&M (I guess it is a point distribution. But really, it is just an M&M). So, you can't have a $p$-value without a distribution, and you can't have a distribution without something repeated (this is not a strictly frequentist argument by the way but more like a nitty gritty computational argument --- see MCMC sampling and the need to repeat for example).
  - Different modes of repetition (sampling, assigning treatment, etc..) combine with different modes of summarizing outcomes (i.e. test statistics) to create distributions.
  - Sometimes we can approximate those distributions really well if we have processes with known characteristics: for example, the Central Limit Theorem says that many of these distributions arising from repetition will look like Normal distributions.

- A good p-value (or a good test) does not mislead: it does not encourage you to overstate your evidence against the claim (i.e. Shout out "significant!" too often in error) (it has a controlled and low false positive rate) nor does it encourage you to believe that there is no evidence against the claim when, in fact, there is evidence (it is statistically powerful). Other ways to put this: it helps you distinguish signal from noise (statistical power) and doesn't mistake noise for signal too often (false positive rate).

- How can we know whether we have a good test (or discover that what we thought was a good test is a bad test)? Well, we have to do a few steps:
  - We have to create some known relationships (otherwise we wouldn't know when the test was giving us a wrong answer). Often we use "no effects" as a our known result because it is easy to create by shuffling the outcomes according to the design.
  - Then we have to repeat the test. (For randomization based tests like the Fisher one using, say, 1000 permutations of the treatment assignment, this test-assessment procedure involves *repeating those 1000 permutations 1000 times* (or so). Since, recall that the test itself generates the null or reference distribution using permutations to create one single p-value. And then we assess the test by repeating **the whole test** with a known result.
  - The proportion of the time that the test produces a p-value below the threshold of rejection or "level" of the test ($\alpha$) is the false positive rate of the test or its "size". So, if we do a bunch of tests of the null of no effects when we make it true and we find 20% rejections at the 5% level, we will have just found out that our test is misleading --- it has a realized false positive rate higher than the promised level.
- We can also repeat the tests with changing true effects to assess power (or errors in the other direction --- errors where there is a true effect but the test misses it).

- Even if we have a well operating test or valid test (a test with controlled false positive rates) with reasonable power, if we start making decisions by looking at many tests, we will quickly start making too many false positive errors again. This problem of "multiple testing" or "multiple comparisons" (from the experimental literature) has lots of interesting solutions: we spent some time talking about adjusting the p-values to control Family Wise Error Rate (FWER) or the probability of making any false positive error across all of the tests we run. 